{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6xLrcBVGL4v"
   },
   "source": [
    "# Chatbot Algorithm\n",
    "\n",
    "The model used in this chatbot is transformer which is first introduced in the paper \"All you need is attention\".\n",
    "\n",
    "I copied the implementation of the model from Tensorflow Website, but made some modifications. \n",
    "\n",
    "The transformer model in Tensorflow Website originally aimed to achieve Sentence-to-Sentence translation. However I used the model to transform user's qeustions or some dialogues to kurius's answer.\n",
    "\n",
    "Simply saying, it aim to achieve Question-to-Answer translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJC9-E6pKcux"
   },
   "source": [
    "## Libarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4884,
     "status": "ok",
     "timestamp": 1611895590078,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "D18pXpR9RCIX"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4875,
     "status": "ok",
     "timestamp": 1611895590079,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "AVx4YVQJqPcL",
    "outputId": "83d1ba06-9ad0-4030-8ed1-d3bfe4ed4b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.4.1\n",
      "tensorflow_datasets version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# Recommend you to use tensorflow version 2.x\n",
    "print(\"tensorflow version: {}\".format(tf.__version__))\n",
    "print(\"tensorflow_datasets version: {}\".format(tfds.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQOvOHXrKjA7"
   },
   "source": [
    "## 1. Load and Preprocess Training data\n",
    "\n",
    "Caution!\n",
    "\n",
    "<font size=\"4\"><b>Training data (.csv or .txt) format</b></font>\n",
    "\n",
    "<b>Correct Format</b>\n",
    "\n",
    "user: question1\n",
    "\n",
    "chatbot: answer1\n",
    "\n",
    "user: question2\n",
    "\n",
    "chatbot: answer2\n",
    "\n",
    "<b>Wrong Format</b>\n",
    "\n",
    "user: question1\n",
    "\n",
    "user: question2\n",
    "\n",
    "chatbot: answer1\n",
    "\n",
    "chatbot: answer2\n",
    "\n",
    "As you saw, one answer should appear after one question!\n",
    "\n",
    "You can check the exact form in \"../Data/dialogue_example.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19517,
     "status": "ok",
     "timestamp": 1611895604728,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "aBuolSaTRR2y",
    "outputId": "4fd9b38c-8b70-4f56-e330-73d5742cd607"
   },
   "outputs": [],
   "source": [
    "# This code is for the user who use google colab.\n",
    "# from google.colab import drive \n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 19511,
     "status": "ok",
     "timestamp": 1611895604729,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "N0S9deQa6n1o"
   },
   "outputs": [],
   "source": [
    "# These path should be modified!\n",
    "# Make sure where your data is or where your model should be saved!\n",
    "# If you use google drive, \"/content/gdrive/My Drive/\"\" may be the root folder.\n",
    "\n",
    "\"\"\"\n",
    "training_data_path = '/content/gdrive/My Drive/Amadeus/Chatbot_core/Data/dialogue_example.txt'\n",
    "emotion_classifier_checkpoint_path = \"/content/gdrive/My Drive/Amadeus/Chatbot_core/Checkpoints/kurisu/emotion_classifier\"\n",
    "chatbot_checkpoint_path = \"/content/gdrive/My Drive/Amadeus/Chatbot_core/Checkpoints/kurisu/chatbot\"\n",
    "\"\"\"\n",
    "\n",
    "training_data_path = './Data/dialogue_example.txt'\n",
    "emotion_classifier_checkpoint_path = \"./Checkpoints/kurisu/emotion_classifier\"\n",
    "chatbot_checkpoint_path = \"./Checkpoints/kurisu/chatbot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 21139,
     "status": "ok",
     "timestamp": 1611895606363,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "0KN5D6mXQWx9"
   },
   "outputs": [],
   "source": [
    "# CSV file should contain three columns, \"name\", \"text\", and \"sentiment\".\n",
    "data = pd.read_csv(training_data_path, sep = '\\t', engine='python')\n",
    "\n",
    "dialogue = list(data['text'].astype(\"string\"))\n",
    "dialogue = list(map(lambda s: re.sub('[!?]', '', s.lower()), dialogue)) # remove !? from original data\n",
    "\n",
    "sent = list(data['sentiment'].astype(\"int\"))\n",
    "sent_labels = list(map(lambda i: np.eye(3)[i], sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVJjkem0VRfi"
   },
   "source": [
    "## Emotion Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21132,
     "status": "ok",
     "timestamp": 1611895606364,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "1WEzKSkOZfX6",
    "outputId": "d00ad458-b89d-45a2-a4c4-115995c8bd0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 28, 4, 48, 10, 26, 16, 29, 40, 42, 51, 22, 30, 20, 68, 36, 61, 4, 58, 31, 60, 65, 25, 9, 13, 57, 45, 30, 4, 67, 24, 73, 43, 117, 40, 40, 80, 87]\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "# Training data for Emotion Classifier\n",
    "# I just used dialogue as the training data, but you can modify sent_dialogue to improve Emotion Classifier.\n",
    "sent_dialogue = dialogue\n",
    "\n",
    "# We need to know max length of dialogues to decide padding size.\n",
    "dial_len = [len(doc) for doc in sent_dialogue]\n",
    "print(dial_len)\n",
    "max_len = max(dial_len)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 21500,
     "status": "ok",
     "timestamp": 1611895606741,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "sjIDsIBGn5B_",
    "outputId": "b300f67a-308d-40e4-dd89-83729211accf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe documentation (about SubwordTextEncoder) says:\\n\\nEncoding is fully invertible because all out-of-vocab wordpieces are byte-encoded\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfds.features.text.SubwordTextEncoder => tfds.deprecated.text.SubwordTextEncoder in higher tensorflow version like 2.4.1\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en for en in dialogue), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_sent = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (sent for sent in sent_dialogue), target_vocab_size = 2**13)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The documentation (about SubwordTextEncoder) says:\n",
    "\n",
    "Encoding is fully invertible because all out-of-vocab wordpieces are byte-encoded\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 21494,
     "status": "ok",
     "timestamp": 1611895606742,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "MdzfzsBmu0nJ"
   },
   "outputs": [],
   "source": [
    "# preprocess and encode given dialogues.\n",
    "# here, we add paddings to make all encoding has same length\n",
    "# This function will be used later to encode user's input!\n",
    "\n",
    "def encode_sent(dialogue):\n",
    "  result = []\n",
    "  for sen in dialogue:\n",
    "    re_sen = re.sub('[.,!?]', '', sen)\n",
    "    temp = tokenizer_sent.encode(re_sen)\n",
    "\n",
    "    if(len(temp) < max_len):\n",
    "      temp = temp + [0] * (max_len - len(temp))\n",
    "      result.append(temp)\n",
    "    else:\n",
    "      print(\"Input length over the max length\")\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 21746,
     "status": "ok",
     "timestamp": 1611895607000,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "GhTdZtbAx6tU"
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "enc_sent_dialogue = encode_sent(sent_dialogue)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# Instead of defining custom \"make_batch\" function, I used tf.data.Dataset to generate mini-batches.\n",
    "sent_dataset = [(enc_sent_dialogue[i], sent_labels[i]) for i in range(len(sent_dialogue))]\n",
    "sent_train_dataset = tf.data.Dataset.from_generator(lambda: sent_dataset, output_types= (tf.int64, tf.int64), output_shapes=([None], [None]))\n",
    "sent_train_dataset = sent_train_dataset.cache()\n",
    "sent_train_dataset = sent_train_dataset.shuffle(BUFFER_SIZE)\n",
    "sent_train_dataset = sent_train_dataset.padded_batch(BATCH_SIZE)\n",
    "sent_train_dataset = sent_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25814,
     "status": "ok",
     "timestamp": 1611895611076,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "yuVEh1fQxrgM",
    "outputId": "f7a71605-2e71-4a3f-bba5-c671e980da08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 2s 132ms/step - loss: 21.1997 - accuracy: 0.3362\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 19.9407 - accuracy: 0.4753\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 27.1142 - accuracy: 0.2809\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 19.1859 - accuracy: 0.4582\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 23.6187 - accuracy: 0.4181\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 26.1366 - accuracy: 0.3660\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 14.1221 - accuracy: 0.4963\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 15.1131 - accuracy: 0.5140\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 8.7194 - accuracy: 0.4529\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 15.4539 - accuracy: 0.3815\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 14.4207 - accuracy: 0.3625\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 13.2472 - accuracy: 0.5418\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 14.2818 - accuracy: 0.5645\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 5.2662 - accuracy: 0.7332\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 997us/step - loss: 6.0013 - accuracy: 0.5471\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 998us/step - loss: 12.1603 - accuracy: 0.3345\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 1000us/step - loss: 5.6315 - accuracy: 0.6289\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 8.8625 - accuracy: 0.5749\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 997us/step - loss: 9.1883 - accuracy: 0.5210\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 7.7360 - accuracy: 0.5645\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 10.0399 - accuracy: 0.5802\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 5.8498 - accuracy: 0.6428\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 6.5973 - accuracy: 0.5838\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.7661 - accuracy: 0.6775\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 5.4279 - accuracy: 0.4759\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 9.1093 - accuracy: 0.5593\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 4.7030 - accuracy: 0.7004\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 6.0888 - accuracy: 0.4497\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.8773 - accuracy: 0.5941\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 4.4505 - accuracy: 0.5890\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 997us/step - loss: 4.7503 - accuracy: 0.6008\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.8517 - accuracy: 0.5817\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 6.7837 - accuracy: 0.4965\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 4.4473 - accuracy: 0.6496\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 4.5338 - accuracy: 0.6582\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.7174 - accuracy: 0.7023\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 9.2008 - accuracy: 0.6658\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 7.0379 - accuracy: 0.4462\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.5426 - accuracy: 0.7177\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.7350 - accuracy: 0.6115\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9877 - accuracy: 0.8327\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 3.7347 - accuracy: 0.6986\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 4.2791 - accuracy: 0.7577\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.6534 - accuracy: 0.7593\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.0286 - accuracy: 0.6638\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 5.2046 - accuracy: 0.5853\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.5286 - accuracy: 0.6796\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 4.6063 - accuracy: 0.6952\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.1835 - accuracy: 0.6428\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 996us/step - loss: 1.4306 - accuracy: 0.7944\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.1334 - accuracy: 0.7838\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.9714 - accuracy: 0.7124\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 4.9053 - accuracy: 0.6533\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.8824 - accuracy: 0.6881\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 5.3755 - accuracy: 0.6097\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.4192 - accuracy: 0.7247\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 3.0962 - accuracy: 0.7212\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.6013 - accuracy: 0.6464\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.9561 - accuracy: 0.7145\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6247 - accuracy: 0.8206\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.5270 - accuracy: 0.6045\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6660 - accuracy: 0.8954\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.5858 - accuracy: 0.7927\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.4010 - accuracy: 0.7072\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.1507 - accuracy: 0.8708\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.6150 - accuracy: 0.8448\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 4.6123 - accuracy: 0.5889\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 997us/step - loss: 3.0260 - accuracy: 0.7230\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.5457 - accuracy: 0.6883\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 4.1136 - accuracy: 0.6584\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.2213 - accuracy: 0.7493\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.0140 - accuracy: 0.7996\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4193 - accuracy: 0.8501\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.8648 - accuracy: 0.8536\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.7075 - accuracy: 0.8902\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5362 - accuracy: 0.7996\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9218 - accuracy: 0.8865\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 997us/step - loss: 1.3809 - accuracy: 0.7456\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 990us/step - loss: 1.3019 - accuracy: 0.8256\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 997us/step - loss: 1.5313 - accuracy: 0.7908\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 999us/step - loss: 1.2262 - accuracy: 0.7160\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.0306 - accuracy: 0.6480\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.1763 - accuracy: 0.8310\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 997us/step - loss: 1.4566 - accuracy: 0.8154\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.1892 - accuracy: 0.8692\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9758 - accuracy: 0.7926\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9675 - accuracy: 0.6673\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.8606 - accuracy: 0.6621\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.9095 - accuracy: 0.6795\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.8030 - accuracy: 0.8083\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2498 - accuracy: 0.8849\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.3714 - accuracy: 0.7579\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.1742 - accuracy: 0.8205\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.2158 - accuracy: 0.7805\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.1945 - accuracy: 0.7318\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 996us/step - loss: 2.0597 - accuracy: 0.6863\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 997us/step - loss: 1.0928 - accuracy: 0.7684\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.2208 - accuracy: 0.7388\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.8554\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.4661 - accuracy: 0.9165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bdc0d53080>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is very simple, so recommend you to modify this part.\n",
    "model = tf.keras.Sequential([\n",
    "                             tf.keras.layers.Dense(128, activation='relu', input_shape=(max_len,)),\n",
    "                             tf.keras.layers.Dropout(0.5),\n",
    "                             tf.keras.layers.Dense(128, activation='relu'), # batch_size, seq_len, 64\n",
    "                             tf.keras.layers.Dropout(0.5),\n",
    "                             tf.keras.layers.Dense(3, activation='softmax') # batch_size, seq_len, 4\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(sent_train_dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 27254,
     "status": "ok",
     "timestamp": 1611895612523,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "MqYFwONX-glZ",
    "outputId": "1cbb5a73-90aa-4175-ef9a-240410674545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Checkpoints/kurisu/emotion_classifier\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Checkpoints/kurisu/emotion_classifier\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nloaded_model = tf.keras.models.load_model(emotion_classifier_checkpoint_path)\\nidx = 27\\ntest = dialogue[idx]\\nprint(test)\\ntest = \"wait running away?\"\\nenc_test = encode_sent([test])\\nanswer = loaded_model.predict(enc_test)\\nprint(np.argmax(answer, axis=1)[0] == np.argmax(sent_labels[idx]))\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(emotion_classifier_checkpoint_path)\n",
    "\n",
    "# You can check whether the model is saved well by uncommenting below codes.\n",
    "\n",
    "\"\"\"\n",
    "loaded_model = tf.keras.models.load_model(emotion_classifier_checkpoint_path)\n",
    "idx = 27\n",
    "test = dialogue[idx]\n",
    "print(test)\n",
    "test = \"wait running away?\"\n",
    "enc_test = encode_sent([test])\n",
    "answer = loaded_model.predict(enc_test)\n",
    "print(np.argmax(answer, axis=1)[0] == np.argmax(sent_labels[idx]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsESiwr2ck_Y"
   },
   "source": [
    "## Preparations for Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 582,
     "status": "ok",
     "timestamp": 1611896796424,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "Ej8a8BursmaH"
   },
   "outputs": [],
   "source": [
    "# Just remember that the model needs to know the start and the end position of the dialogue\n",
    "# Thus, we define the start and the end position as tokenizer_en.vocab_size and tokenizer_en.vocab_size+1, respectively.\n",
    "# vocab_size is the max number of vocabulary (depending on the training dataset)\n",
    "# encoded data has sequence of numbers => vocab_size and vocab_size + 1 will inform the model what is start and what is end among the numbers. \n",
    "\n",
    "def encode(dialogue):\n",
    "  result = []\n",
    "  for en in dialogue:\n",
    "    result.append([tokenizer_en.vocab_size] + tokenizer_en.encode(en) + [tokenizer_en.vocab_size+1])\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1611896798516,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "SH56eU16vA1A"
   },
   "outputs": [],
   "source": [
    "dialogue_encode = encode(dialogue)\n",
    "\n",
    "input_dataset = []\n",
    "target_dataset = []\n",
    "\n",
    "for i in range(len(dialogue_encode)):\n",
    "  if i == len(dialogue_encode):\n",
    "    break\n",
    "  if i%2 != 0:\n",
    "    continue\n",
    "  # As I mentioned at the beginning, one question and one answer relationship is applied.\n",
    "  input_dataset.append(dialogue_encode[i])\n",
    "  target_dataset.append(dialogue_encode[i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1611896835514,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "et9fplAmRgvJ",
    "outputId": "47533908-c304-477d-b848-f2a20bc5413e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# If the dataset has correct format, the length of input and target dataset should be same.\n",
    "print(len(input_dataset))\n",
    "print(len(target_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1611896860411,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "KtYEyyhzx8k5"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# Instead of defining custom \"make_batch\" function, I used tf.data.Dataset to generate mini-batches.\n",
    "sum_dataset = [(input_dataset[i], target_dataset[i]) for i in range(len(input_dataset))]\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: sum_dataset, output_types= (tf.int64, tf.int64), output_shapes=([None], [None]))\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FwZmdlkimRi"
   },
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 1252,
     "status": "ok",
     "timestamp": 1611898209700,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "GGPapv_O175E"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "# Position encoding gives the model position information of the input dialogue.\n",
    "# For example, there is an input dialogue \"your apple and my apple\"\n",
    "# apple appears twice in the dialogue. How the model can differentiate them?\n",
    "# we can multiply different weights (sine and cosine in this example) to different position\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# padding_mask, look_ahead_mask is used for masking target data because the model need to predict next words without looking at the target data.\n",
    "# Specifically, we just assign (or padding) 0 to the target data by multiplying zero-vector.\n",
    "# By doing this, we can hide target data from the model's calculation.\n",
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  # What is q, k, v?\n",
    "  # Actually, these three varaibles are the same vector which represents same input dialogue.\n",
    "\n",
    "  # Why do we use three same variables?\n",
    "  # Simply saying, there are some useful relationships (like correlation) among the words in the dialogue.\n",
    "\n",
    "  # How to get such relationships?\n",
    "  # Using formula, Attention(Q,K,V) = softmax (QK^T/sqrt(dk))V => The result is attention weights\n",
    "\n",
    "  # following code is just the implementation of the above formula.\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * int(-1e9)) # mask the future(target) data\n",
    "\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) # num_heads는 논문에서 8개로 8개의 attention을 생성 d_model(=512) = num_heads(=8) * seq_len_q(k, v) 이므로 seq_len_q(k,v)는 64의 값을 가진다.\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model) 1 60 512\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth) 1 8 60 64\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention( # 1 8 60 60 -> 1 8 60 64 multiply Transpose(1 8 60 64) = 1 8 64 60 (np.transpose(x, [0,1,3,2])) 따라서 60x64 multiply 64x60이므로 60 60이 맞음\n",
    "        q, k, v, mask)\n",
    "  \n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights\n",
    "\n",
    "# model에서 feedforward 부분이며 결과적으로 output이 d_model이 되어 나가도록 한다. (다음 encoding layer에서도 써야하므로)\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "# 참고로 Encoder와 EncoderLayer의 차이는, Encoder는 EncoderLayer를 여러개 가지고 있다. Decoder 또한 DecoderLayer를 여러개 가지고 있기 때문에 서로 클래스를 분리했다.\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    # two sublayers\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    # Add & Norm after each sublayer\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model) # 어차피 embedding이라는 것은 어떤 특별히 만들어진 layer에 들어가서 훈련도중 나오는 weight임을 알고있음. (skip-gram 처럼)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1611898210714,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "A5bHHdHs3tKT"
   },
   "outputs": [],
   "source": [
    "# hyper parameters, paper에 다른 버전들도 나와있다고 하니 참고할 수 있음.\n",
    "# 4 128 512 8 0.1\n",
    "# 6 1024 4096 16 0.3\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "\"\"\"\n",
    "num_layers = 6\n",
    "d_model = 1024\n",
    "dff = 4096\n",
    "num_heads = 16\n",
    "dropout_rate = 0.3\n",
    "\"\"\"\n",
    "\n",
    "input_vocab_size = tokenizer_en.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1611898227804,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "-Nlpcp2a39Nr"
   },
   "outputs": [],
   "source": [
    "# 이것 역시 paper에 나온 공식에 따라 AdamOptimizer의 learning schedule을 custom함.\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1611898231625,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "vKkrbCjU4Dcs"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "# Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy( # two or more labels가 있을 때 사용하라는데 거기에 integer value. 만약 one-hot encoding이라면 CategoricalCrossentropy사용. 여기선 tokenizer에 의해 integer를 가지므로 이거 쓰는듯.\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask # real이 0이 아닐 때, pred와의 loss만을 계산한다.\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask) # mask의 sum이 곧 valid prediction의 개수이므로 평균 내기 위해서 나눔\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1611898233138,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "IOhIowVU4HXD"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1611898233899,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "OL81IW-J4W7E"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar): # inp = input, tar = target\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1611898235906,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "5ipnDUSv0OTs",
    "outputId": "e63f7ec5-4309-46bf-fe58-ccb499eed483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, chatbot_checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1611898269563,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "lzffUU2j4h8s"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:] # shifted tar_input (+ 1)\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(tf.cast(inp, dtype=tf.int64), tf.cast(tar_inp,dtype=tf.int64), \n",
    "                                 tf.cast(True, dtype=tf.bool), \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZ6L15Fvn9OA"
   },
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45731,
     "status": "ok",
     "timestamp": 1611898341514,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "hGJQCWQb4mbs",
    "outputId": "2610d3d8-7aa2-417d-d97f-074f182b4f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.0180 Accuracy 0.7222\n",
      "Epoch 1 Loss 0.0197 Accuracy 0.6936\n",
      "Time taken for 1 epoch: 12.0969877243042 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0077 Accuracy 0.5312\n",
      "Epoch 2 Loss 0.0221 Accuracy 0.6319\n",
      "Time taken for 1 epoch: 0.16054463386535645 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0202 Accuracy 0.6471\n",
      "Epoch 3 Loss 0.0367 Accuracy 0.6754\n",
      "Time taken for 1 epoch: 0.14261889457702637 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0200 Accuracy 0.5000\n",
      "Epoch 4 Loss 0.0206 Accuracy 0.6913\n",
      "Time taken for 1 epoch: 0.15059685707092285 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0021 Accuracy 0.6731\n",
      "Epoch 5 Loss 0.0573 Accuracy 0.6538\n",
      "Time taken for 1 epoch: 0.14960026741027832 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0088 Accuracy 0.6250\n",
      "Epoch 6 Loss 0.0321 Accuracy 0.6220\n",
      "Time taken for 1 epoch: 0.1326448917388916 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.2819 Accuracy 0.6731\n",
      "Epoch 7 Loss 0.0747 Accuracy 0.6212\n",
      "Time taken for 1 epoch: 0.13364481925964355 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0243 Accuracy 0.8971\n",
      "Epoch 8 Loss 0.0171 Accuracy 0.6877\n",
      "Time taken for 1 epoch: 0.13364100456237793 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0668 Accuracy 0.5469\n",
      "Epoch 9 Loss 0.0324 Accuracy 0.6224\n",
      "Time taken for 1 epoch: 0.1376328468322754 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.2194 Accuracy 0.7222\n",
      "Epoch 10 Loss 0.0853 Accuracy 0.6755\n",
      "Time taken for 1 epoch: 0.12865614891052246 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0071 Accuracy 0.8500\n",
      "Epoch 11 Loss 0.0606 Accuracy 0.6486\n",
      "Time taken for 1 epoch: 0.13366270065307617 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0361 Accuracy 0.6176\n",
      "Epoch 12 Loss 0.0694 Accuracy 0.6313\n",
      "Time taken for 1 epoch: 0.13164758682250977 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0586 Accuracy 0.7083\n",
      "Epoch 13 Loss 0.0418 Accuracy 0.6645\n",
      "Time taken for 1 epoch: 0.12566542625427246 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0046 Accuracy 0.5469\n",
      "Epoch 14 Loss 0.1313 Accuracy 0.6879\n",
      "Time taken for 1 epoch: 0.13463854789733887 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0018 Accuracy 0.4783\n",
      "Epoch 15 Loss 0.0551 Accuracy 0.6465\n",
      "Time taken for 1 epoch: 0.13164758682250977 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0879 Accuracy 0.8250\n",
      "Epoch 16 Loss 0.0571 Accuracy 0.6777\n",
      "Time taken for 1 epoch: 0.12765932083129883 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0094 Accuracy 0.8000\n",
      "Epoch 17 Loss 0.0476 Accuracy 0.7270\n",
      "Time taken for 1 epoch: 0.12765836715698242 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0117 Accuracy 0.6413\n",
      "Epoch 18 Loss 0.0227 Accuracy 0.6551\n",
      "Time taken for 1 epoch: 0.14062738418579102 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0304 Accuracy 0.6406\n",
      "Epoch 19 Loss 0.0514 Accuracy 0.6193\n",
      "Time taken for 1 epoch: 0.1386260986328125 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0228 Accuracy 0.6806\n",
      "Epoch 20 Loss 0.0236 Accuracy 0.7031\n",
      "Time taken for 1 epoch: 0.12863421440124512 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0128 Accuracy 0.6944\n",
      "Epoch 21 Loss 0.0343 Accuracy 0.6476\n",
      "Time taken for 1 epoch: 0.1296544075012207 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0068 Accuracy 0.5543\n",
      "Epoch 22 Loss 0.0194 Accuracy 0.6656\n",
      "Time taken for 1 epoch: 0.13065075874328613 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0129 Accuracy 0.7500\n",
      "Epoch 23 Loss 0.0197 Accuracy 0.6529\n",
      "Time taken for 1 epoch: 0.13763046264648438 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0032 Accuracy 0.5156\n",
      "Epoch 24 Loss 0.0282 Accuracy 0.6561\n",
      "Time taken for 1 epoch: 0.1296541690826416 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0148 Accuracy 0.7778\n",
      "Epoch 25 Loss 0.1439 Accuracy 0.6613\n",
      "Time taken for 1 epoch: 0.12765836715698242 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0162 Accuracy 0.6176\n",
      "Epoch 26 Loss 0.0124 Accuracy 0.6520\n",
      "Time taken for 1 epoch: 0.12862396240234375 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0104 Accuracy 0.5192\n",
      "Epoch 27 Loss 0.0269 Accuracy 0.6438\n",
      "Time taken for 1 epoch: 0.12963056564331055 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0309 Accuracy 0.6324\n",
      "Epoch 28 Loss 0.0370 Accuracy 0.6319\n",
      "Time taken for 1 epoch: 0.14960098266601562 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0029 Accuracy 0.7500\n",
      "Epoch 29 Loss 0.1046 Accuracy 0.6847\n",
      "Time taken for 1 epoch: 0.1296520233154297 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0197 Accuracy 0.7059\n",
      "Epoch 30 Loss 0.0552 Accuracy 0.6205\n",
      "Time taken for 1 epoch: 0.13523268699645996 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0056 Accuracy 0.5109\n",
      "Epoch 31 Loss 0.2102 Accuracy 0.6612\n",
      "Time taken for 1 epoch: 0.1376345157623291 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0409 Accuracy 0.5326\n",
      "Epoch 32 Loss 0.0956 Accuracy 0.6364\n",
      "Time taken for 1 epoch: 0.15192174911499023 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0033 Accuracy 0.5652\n",
      "Epoch 33 Loss 0.1162 Accuracy 0.6258\n",
      "Time taken for 1 epoch: 0.1326453685760498 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0382 Accuracy 0.7750\n",
      "Epoch 34 Loss 0.1149 Accuracy 0.7374\n",
      "Time taken for 1 epoch: 0.13968586921691895 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0596 Accuracy 0.5000\n",
      "Epoch 35 Loss 0.0464 Accuracy 0.6404\n",
      "Time taken for 1 epoch: 0.13962697982788086 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0250 Accuracy 0.6250\n",
      "Epoch 36 Loss 0.0545 Accuracy 0.6404\n",
      "Time taken for 1 epoch: 0.14662957191467285 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0393 Accuracy 0.5441\n",
      "Epoch 37 Loss 0.0933 Accuracy 0.6119\n",
      "Time taken for 1 epoch: 0.1366128921508789 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0030 Accuracy 0.6094\n",
      "Epoch 38 Loss 0.0092 Accuracy 0.6420\n",
      "Time taken for 1 epoch: 0.1496262550354004 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.1123 Accuracy 0.6087\n",
      "Epoch 39 Loss 0.0331 Accuracy 0.6592\n",
      "Time taken for 1 epoch: 0.13663291931152344 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0167 Accuracy 0.6944\n",
      "Epoch 40 Loss 0.0422 Accuracy 0.6277\n",
      "Time taken for 1 epoch: 0.14364004135131836 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0201 Accuracy 0.6250\n",
      "Epoch 41 Loss 0.0165 Accuracy 0.6438\n",
      "Time taken for 1 epoch: 0.1425948143005371 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0097 Accuracy 0.6522\n",
      "Epoch 42 Loss 0.0110 Accuracy 0.6409\n",
      "Time taken for 1 epoch: 0.13366460800170898 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0126 Accuracy 0.7065\n",
      "Epoch 43 Loss 0.0122 Accuracy 0.7007\n",
      "Time taken for 1 epoch: 0.12825512886047363 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0012 Accuracy 0.6389\n",
      "Epoch 44 Loss 0.0249 Accuracy 0.6530\n",
      "Time taken for 1 epoch: 0.13927865028381348 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0285 Accuracy 0.6042\n",
      "Epoch 45 Loss 0.0468 Accuracy 0.6677\n",
      "Time taken for 1 epoch: 0.13663244247436523 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0395 Accuracy 0.6176\n",
      "Epoch 46 Loss 0.0397 Accuracy 0.6288\n",
      "Time taken for 1 epoch: 0.1296534538269043 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0054 Accuracy 0.7361\n",
      "Epoch 47 Loss 0.0702 Accuracy 0.6277\n",
      "Time taken for 1 epoch: 0.12967157363891602 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0049 Accuracy 0.7188\n",
      "Epoch 48 Loss 0.0750 Accuracy 0.6623\n",
      "Time taken for 1 epoch: 0.1416032314300537 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0719 Accuracy 0.6196\n",
      "Epoch 49 Loss 0.0466 Accuracy 0.6645\n",
      "Time taken for 1 epoch: 0.1351778507232666 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0902 Accuracy 0.5938\n",
      "Saving checkpoint for epoch 50 at ./Checkpoints/kurisu/chatbot\\ckpt-13\n",
      "test\n",
      "Epoch 50 Loss 0.0999 Accuracy 0.6185\n",
      "Time taken for 1 epoch: 0.601205587387085 secs\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0104 Accuracy 0.5652\n",
      "Epoch 51 Loss 0.1423 Accuracy 0.6227\n",
      "Time taken for 1 epoch: 0.1367194652557373 secs\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0440 Accuracy 0.7361\n",
      "Epoch 52 Loss 0.2187 Accuracy 0.6242\n",
      "Time taken for 1 epoch: 0.15044665336608887 secs\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0682 Accuracy 0.6029\n",
      "Epoch 53 Loss 0.0733 Accuracy 0.6570\n",
      "Time taken for 1 epoch: 0.14656662940979004 secs\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.2675 Accuracy 0.5278\n",
      "Epoch 54 Loss 0.1921 Accuracy 0.5714\n",
      "Time taken for 1 epoch: 0.14534449577331543 secs\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0390 Accuracy 0.8594\n",
      "Epoch 55 Loss 0.1030 Accuracy 0.6558\n",
      "Time taken for 1 epoch: 0.1468830108642578 secs\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0162 Accuracy 0.5833\n",
      "Epoch 56 Loss 0.0508 Accuracy 0.6280\n",
      "Time taken for 1 epoch: 0.15529489517211914 secs\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0824 Accuracy 0.7115\n",
      "Epoch 57 Loss 0.1508 Accuracy 0.6123\n",
      "Time taken for 1 epoch: 0.14586257934570312 secs\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0137 Accuracy 0.5000\n",
      "Epoch 58 Loss 0.0347 Accuracy 0.6487\n",
      "Time taken for 1 epoch: 0.14571523666381836 secs\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.1981 Accuracy 0.5652\n",
      "Epoch 59 Loss 0.0789 Accuracy 0.6301\n",
      "Time taken for 1 epoch: 0.14358854293823242 secs\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0376 Accuracy 0.7083\n",
      "Epoch 60 Loss 0.0513 Accuracy 0.6395\n",
      "Time taken for 1 epoch: 0.14464449882507324 secs\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0110 Accuracy 0.7188\n",
      "Epoch 61 Loss 0.0274 Accuracy 0.6832\n",
      "Time taken for 1 epoch: 0.15282607078552246 secs\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0417 Accuracy 0.7083\n",
      "Epoch 62 Loss 0.0361 Accuracy 0.6743\n",
      "Time taken for 1 epoch: 0.13995790481567383 secs\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0083 Accuracy 0.8125\n",
      "Epoch 63 Loss 0.0236 Accuracy 0.6398\n",
      "Time taken for 1 epoch: 0.1416187286376953 secs\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0242 Accuracy 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Loss 0.0246 Accuracy 0.6308\n",
      "Time taken for 1 epoch: 0.14162111282348633 secs\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0104 Accuracy 0.6196\n",
      "Epoch 65 Loss 0.0235 Accuracy 0.6926\n",
      "Time taken for 1 epoch: 0.13663387298583984 secs\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0183 Accuracy 0.6522\n",
      "Epoch 66 Loss 0.0151 Accuracy 0.6280\n",
      "Time taken for 1 epoch: 0.13664555549621582 secs\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0210 Accuracy 0.5588\n",
      "Epoch 67 Loss 0.0214 Accuracy 0.6175\n",
      "Time taken for 1 epoch: 0.14760589599609375 secs\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.0033 Accuracy 0.7031\n",
      "Epoch 68 Loss 0.0131 Accuracy 0.6235\n",
      "Time taken for 1 epoch: 0.1436159610748291 secs\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0217 Accuracy 0.7115\n",
      "Epoch 69 Loss 0.0316 Accuracy 0.6280\n",
      "Time taken for 1 epoch: 0.13663387298583984 secs\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0315 Accuracy 0.5441\n",
      "Epoch 70 Loss 0.0118 Accuracy 0.6113\n",
      "Time taken for 1 epoch: 0.13065147399902344 secs\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.0143 Accuracy 0.7031\n",
      "Epoch 71 Loss 0.0084 Accuracy 0.7017\n",
      "Time taken for 1 epoch: 0.12865519523620605 secs\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.0392 Accuracy 0.6912\n",
      "Epoch 72 Loss 0.0128 Accuracy 0.7113\n",
      "Time taken for 1 epoch: 0.1305835247039795 secs\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.0018 Accuracy 0.7917\n",
      "Epoch 73 Loss 0.0093 Accuracy 0.6581\n",
      "Time taken for 1 epoch: 0.1326463222503662 secs\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.0026 Accuracy 0.7083\n",
      "Epoch 74 Loss 0.0111 Accuracy 0.6571\n",
      "Time taken for 1 epoch: 0.1356370449066162 secs\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.0246 Accuracy 0.7917\n",
      "Epoch 75 Loss 0.0116 Accuracy 0.6890\n",
      "Time taken for 1 epoch: 0.13065075874328613 secs\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.0015 Accuracy 0.5156\n",
      "Epoch 76 Loss 0.0084 Accuracy 0.6113\n",
      "Time taken for 1 epoch: 0.13464570045471191 secs\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.0009 Accuracy 0.7344\n",
      "Epoch 77 Loss 0.0088 Accuracy 0.6398\n",
      "Time taken for 1 epoch: 0.12865161895751953 secs\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.0006 Accuracy 0.7750\n",
      "Epoch 78 Loss 0.0092 Accuracy 0.6867\n",
      "Time taken for 1 epoch: 0.12865471839904785 secs\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.0219 Accuracy 0.6250\n",
      "Epoch 79 Loss 0.0091 Accuracy 0.6113\n",
      "Time taken for 1 epoch: 0.13260483741760254 secs\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.0014 Accuracy 0.6250\n",
      "Epoch 80 Loss 0.0079 Accuracy 0.6449\n",
      "Time taken for 1 epoch: 0.13564682006835938 secs\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.0005 Accuracy 0.7031\n",
      "Epoch 81 Loss 0.0073 Accuracy 0.6592\n",
      "Time taken for 1 epoch: 0.13064074516296387 secs\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.0003 Accuracy 0.7500\n",
      "Epoch 82 Loss 0.0084 Accuracy 0.7138\n",
      "Time taken for 1 epoch: 0.13366222381591797 secs\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.0004 Accuracy 0.5326\n",
      "Epoch 83 Loss 0.0051 Accuracy 0.6292\n",
      "Time taken for 1 epoch: 0.1394033432006836 secs\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.0008 Accuracy 0.6154\n",
      "Epoch 84 Loss 0.0053 Accuracy 0.6369\n",
      "Time taken for 1 epoch: 0.13364219665527344 secs\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.0003 Accuracy 0.6196\n",
      "Epoch 85 Loss 0.0071 Accuracy 0.6350\n",
      "Time taken for 1 epoch: 0.13464069366455078 secs\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.0146 Accuracy 0.7885\n",
      "Epoch 86 Loss 0.0075 Accuracy 0.6273\n",
      "Time taken for 1 epoch: 0.14959931373596191 secs\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.0004 Accuracy 0.7083\n",
      "Epoch 87 Loss 0.0095 Accuracy 0.5988\n",
      "Time taken for 1 epoch: 0.14162087440490723 secs\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.0002 Accuracy 0.6346\n",
      "Epoch 88 Loss 0.0053 Accuracy 0.6809\n",
      "Time taken for 1 epoch: 0.1466071605682373 secs\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.0101 Accuracy 0.6522\n",
      "Epoch 89 Loss 0.0071 Accuracy 0.7340\n",
      "Time taken for 1 epoch: 0.13128995895385742 secs\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.0002 Accuracy 0.7115\n",
      "Epoch 90 Loss 0.0046 Accuracy 0.6946\n",
      "Time taken for 1 epoch: 0.13940691947937012 secs\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.0006 Accuracy 0.7045\n",
      "Epoch 91 Loss 0.0072 Accuracy 0.6389\n",
      "Time taken for 1 epoch: 0.13051676750183105 secs\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.0109 Accuracy 0.7361\n",
      "Epoch 92 Loss 0.0054 Accuracy 0.6993\n",
      "Time taken for 1 epoch: 0.14760637283325195 secs\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.0006 Accuracy 0.7917\n",
      "Epoch 93 Loss 0.0059 Accuracy 0.6854\n",
      "Time taken for 1 epoch: 0.12865543365478516 secs\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.0002 Accuracy 0.7500\n",
      "Epoch 94 Loss 0.0065 Accuracy 0.6821\n",
      "Time taken for 1 epoch: 0.1296534538269043 secs\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.0002 Accuracy 0.9000\n",
      "Epoch 95 Loss 0.0120 Accuracy 0.6832\n",
      "Time taken for 1 epoch: 0.12267231941223145 secs\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.0167 Accuracy 0.6458\n",
      "Epoch 96 Loss 0.0082 Accuracy 0.6489\n",
      "Time taken for 1 epoch: 0.13064980506896973 secs\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.0231 Accuracy 0.6029\n",
      "Epoch 97 Loss 0.0126 Accuracy 0.6438\n",
      "Time taken for 1 epoch: 0.1376478672027588 secs\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.0003 Accuracy 0.6087\n",
      "Epoch 98 Loss 0.0084 Accuracy 0.6254\n",
      "Time taken for 1 epoch: 0.13364219665527344 secs\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.0003 Accuracy 0.5972\n",
      "Epoch 99 Loss 0.0067 Accuracy 0.6017\n",
      "Time taken for 1 epoch: 0.13464021682739258 secs\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.0002 Accuracy 0.5625\n",
      "Saving checkpoint for epoch 100 at ./Checkpoints/kurisu/chatbot\\ckpt-14\n",
      "test\n",
      "Epoch 100 Loss 0.0080 Accuracy 0.6498\n",
      "Time taken for 1 epoch: 0.7860100269317627 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 50 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    print(\"test\")\n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7mriR2Ho6LX"
   },
   "source": [
    "## Inference Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1611898517620,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "XXQsbVag42U8"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  start_token = [tokenizer_en.vocab_size]\n",
    "  end_token = [tokenizer_en.vocab_size + 1]\n",
    "  \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "  # 최대 길이는 몇일지 모르겠다.\n",
    "  for i in range(200):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == tokenizer_en.vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1611898517621,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "limZuXm9FGmI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, attention_weights = evaluate(re.sub('[!?]', '', sentence.lower()))\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in  result \n",
    "                                            if i < tokenizer_en.vocab_size]) \n",
    "  predicted_sentence = predicted_sentence.split('.')\n",
    "  predicted_sentence = [w.strip() for w in predicted_sentence]\n",
    "  predicted_sentence = [w.capitalize() for w in predicted_sentence]\n",
    "  predicted_sentence = '. '.join(predicted_sentence)\n",
    "\n",
    "  print('Input: {}'.format(re.sub('[!?]', '', sentence.lower())))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2099,
     "status": "ok",
     "timestamp": 1611898520143,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "eO7r66o7FHqn",
    "outputId": "fd7e05ed-6852-4e9f-af21-dfd915ebfbf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: who are you\n",
      "훈련시 사용된 크리스의 대화: that's what i'd like to know\n",
      "Input: are you\n",
      "Predicted translation: That's what i'd like to know\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[0])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[1])\n",
    "translate(\"are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133903,
     "status": "ok",
     "timestamp": 1611895719288,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "KZRxobeP-7xo",
    "outputId": "c63dd9d5-70f8-4ba0-a0ef-185203841a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: what\n",
      "훈련시 사용된 크리스의 대화: back there, you were going to tell me something.\n",
      "Input: what\n",
      "Predicted translation: Back there, you were going to tell me something. \n"
     ]
    }
   ],
   "source": [
    " print(\"훈련시 사용된 오카베의 대화: \" + dialogue[2])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[3])\n",
    "translate(\"What?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1509,
     "status": "ok",
     "timestamp": 1611885912856,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "T1zTPFyi-75k",
    "outputId": "77cdd989-7bfa-4451-c84b-b26cb2f4e5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: back there\n",
      "훈련시 사용된 크리스의 대화: about fifteen minutes ago.\n",
      "Input: back there\n",
      "Predicted translation: About fifteen minutes ago. \n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[4])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[5])\n",
    "translate(\"Back there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2898,
     "status": "ok",
     "timestamp": 1611885918814,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "RdCqi5Jy_c0J",
    "outputId": "5a024174-d894-4615-be2e-2ef9aa301b28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: makise... kurisu\n",
      "훈련시 사용된 크리스의 대화: i'm surprised you know of it.\n",
      "Input: kurisu\n",
      "Predicted translation: Well then, let's change the format of this lecture to a discussion. \n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[6])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[7])\n",
    "translate(\"kurisu?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 1628,
     "status": "ok",
     "timestamp": 1593799941393,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "3e0hn9Qb_mi_",
    "outputId": "2a62ed27-fda3-4241-e4f7-6afc4e0430c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: you are you an agent of the organization\n",
      "훈련시 사용된 크리스의 대화: organization what i just wanted ask you...\n",
      "Input: you an agent of organization\n",
      "Predicted translation: Organization what i just wanted ask you. . . \n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[8])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[9])\n",
    "translate(\"You! an agent of Organization?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1593806254588,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "ePPRxK8O_0Qd",
    "outputId": "693834f8-4189-45f5-a7aa-336e9bce38ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: it's me. i've been caught by an organization agent.\n",
      "훈련시 사용된 크리스의 대화: who are you talking to\n",
      "Input: i've been caught by an agent.\n",
      "Predicted translation: Who are you talking to\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[10])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[11])\n",
    "translate(\"I've been caught by an agent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1815,
     "status": "ok",
     "timestamp": 1611885946132,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "idpb99gT_9IY",
    "outputId": "30872608-7ea6-4fae-bdce-9cad34779641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: yeah, it should be no trouble.\n",
      "훈련시 사용된 크리스의 대화: huh it's turned off.\n",
      "Input: yeah, no trouble\n",
      "Predicted translation: Huh it's turned off. \n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[12])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[13])\n",
    "translate(\"Yeah, no trouble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1591,
     "status": "ok",
     "timestamp": 1611885949771,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "Uj3lAdgOAEep",
    "outputId": "2e352d85-ecbb-4696-b5cd-9993ba85e0a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: i'll tell you a secret. if anyone but me touches it, it deactivates.\n",
      "훈련시 사용된 크리스의 대화: i see. you were talking to yourself.\n",
      "Input: if anyone except me touches it, it would be off\n",
      "Predicted translation: Who are you talking to\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[14])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[15])\n",
    "translate(\"If anyone except me touches it, it would be off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 774,
     "status": "ok",
     "timestamp": 1611885963234,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "AQg3pMfoAbw8",
    "outputId": "8852b585-37ba-49d1-86ea-6ec092b23125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: genius girl, when next we meet, we will be enemies. farewell.\n",
      "훈련시 사용된 크리스의 대화: wait\n",
      "Input: genius girl, next time we will be enemies. farewell.\n",
      "Predicted translation: Wait\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[16])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[17])\n",
    "translate(\"genius girl, next time we will be enemies. Farewell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1931,
     "status": "ok",
     "timestamp": 1611885968385,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "i4n1N8xUFO_H",
    "outputId": "d8c078ef-2393-4e7d-8f8c-24c25891dbed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: how i can feel her. she's really here. you're not a spirit\n",
      "훈련시 사용된 크리스의 대화: do you want me to call the cops\n",
      "Input: how i can feel her she is here.\n",
      "Predicted translation: Do you want me to call the cops\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[18])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[19])\n",
    "translate(\"how i can feel her? she is here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3064,
     "status": "ok",
     "timestamp": 1611885975203,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "pPp0TmGylAnT",
    "outputId": "7dcf0b13-8490-459d-a30d-581300d3ec87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: i just want to know the truth you were stabbed right here...\n",
      "훈련시 사용된 크리스의 대화: what do you mean by \"the truth\" are you stupid do you want to die\n",
      "Input: i want to know truth you were stabbed...\n",
      "Predicted translation: What do you mean by \"the truth\" are you stupid do you want to die\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[20])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[21])\n",
    "translate(\"i want to know truth you were stabbed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1450,
     "status": "ok",
     "timestamp": 1611885989941,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "mnDiT-kplWea",
    "outputId": "a403121c-c37e-4906-fa6a-a102f4b7a681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: wait are you running away\n",
      "훈련시 사용된 크리스의 대화: you're...\n",
      "Input: are you running away\n",
      "Predicted translation: You're. . . \n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[22])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[23])\n",
    "translate(\"are you running away?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 2109,
     "status": "ok",
     "timestamp": 1593797471570,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "Ha_O_dedllYF",
    "outputId": "cd5c81bc-387d-43ba-ebfc-571f0be0f79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: time machines\n",
      "훈련시 사용된 크리스의 대화: i think time machines are nothing more than a pipe dream.\n",
      "Input: time machines\n",
      "Predicted translation: I think time machines are nothing more than a pipe dream. \n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[24])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[25])\n",
    "translate(\"time machines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 3313,
     "status": "ok",
     "timestamp": 1593799967181,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "feCf_nGElvNq",
    "outputId": "db9a17ea-6ae5-4c14-f8da-3610fb563057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: do you think one could build a time machine\n",
      "훈련시 사용된 크리스의 대화: my personal belief is that time machines are not possible. however, that is not to say they are impossible altogether\n",
      "Input: do you think one could build time machine\n",
      "Predicted translation: My personal belief is that time machines are not possible. However, that is not to say they are impossible altogether\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[32])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[33])\n",
    "translate(\"do you think one could build time machine?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2593,
     "status": "ok",
     "timestamp": 1611879550435,
     "user": {
      "displayName": "­김지수 / 학생 / 컴퓨터공학부",
      "photoUrl": "",
      "userId": "09232858552890099739"
     },
     "user_tz": -540
    },
    "id": "3OaAq57Lu4HU",
    "outputId": "b9fa19da-3b72-4f73-ad14-b938463f1bfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련시 사용된 오카베의 대화: there are a number of time travel theories but they are nothing but speculation.\n",
      "훈련시 사용된 크리스의 대화: even so, we could be missing that one critical discovery that unlocks new understanding\n",
      "Input: a number of time travel series exist but they are nothing but speculation\n",
      "Predicted translation: Even so, we could be missing that one critical discovery that unlocks new understanding\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련시 사용된 오카베의 대화: \" + dialogue[36])\n",
    "print(\"훈련시 사용된 크리스의 대화: \" + dialogue[37])\n",
    "translate(\"a number of time travel series exist but they are nothing but speculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9MzARjm4YOo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyORiqDFOpXSrQ2EC1YFextr",
   "collapsed_sections": [],
   "name": "Chatbot_application.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
